{
  "doc_meta": {
    "id": "AI-001",
    "version": "2025-01-14",
    "owners": ["pablo"],
    "scope": ["ai", "openai", "gpt-4o", "anthropic", "google", "azure"],
    "status": "active",
    "related": ["API-001", "COMP-001", "ARCH-001"]
  }
}

# StayPost AI í†µí•© ê°€ì´ë“œ

ì´ ë¬¸ì„œëŠ” StayPost í”„ë¡œì íŠ¸ì˜ AI ì„œë¹„ìŠ¤ í†µí•© ë°©ë²•ê³¼ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­ì„ ì„¤ëª…í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ AI ì„œë¹„ìŠ¤ë¥¼ ëª¨ë“ˆí™”í•˜ì—¬ ì‰½ê²Œ ì¶”ê°€í•˜ê±°ë‚˜ êµì²´í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨
- [ê°œìš”](#ê°œìš”)
- [AI ì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜](#ai-ì„œë¹„ìŠ¤-ì•„í‚¤í…ì²˜)
- [AI ì„œë¹„ìŠ¤ ì¸í„°í˜ì´ìŠ¤](#ai-ì„œë¹„ìŠ¤-ì¸í„°í˜ì´ìŠ¤)
- [ì§€ì› AI ì„œë¹„ìŠ¤](#ì§€ì›-ai-ì„œë¹„ìŠ¤)
- [ìƒˆë¡œìš´ AI ì„œë¹„ìŠ¤ ì¶”ê°€ ê°€ì´ë“œ](#ìƒˆë¡œìš´-ai-ì„œë¹„ìŠ¤-ì¶”ê°€-ê°€ì´ë“œ)
- [í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§](#í”„ë¡¬í”„íŠ¸-ì—”ì§€ë‹ˆì–´ë§)
- [ì´ë¯¸ì§€ ë¶„ì„](#ì´ë¯¸ì§€-ë¶„ì„)
- [ì—ëŸ¬ ì²˜ë¦¬](#ì—ëŸ¬-ì²˜ë¦¬)
- [ì„±ëŠ¥ ìµœì í™”](#ì„±ëŠ¥-ìµœì í™”)
- [ë¹„ìš© ê´€ë¦¬](#ë¹„ìš©-ê´€ë¦¬)
- [ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…](#ëª¨ë‹ˆí„°ë§-ë°-ë¡œê¹…)

## ê°œìš”

StayPostëŠ” ë‹¤ì–‘í•œ AI ì„œë¹„ìŠ¤ë¥¼ í†µí•©í•˜ì—¬ ê°ì • ê¸°ë°˜ ìº¡ì…˜ ìƒì„±ê³¼ ì´ë¯¸ì§€ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ëª¨ë“ˆí™”ëœ ì•„í‚¤í…ì²˜ë¥¼ í†µí•´ ìƒˆë¡œìš´ AI ì„œë¹„ìŠ¤ë¥¼ ì‰½ê²Œ ì¶”ê°€í•˜ê±°ë‚˜ ê¸°ì¡´ ì„œë¹„ìŠ¤ë¥¼ êµì²´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ì£¼ìš” íŠ¹ì§•
- ğŸ”Œ **í”ŒëŸ¬ê·¸ì¸ ì•„í‚¤í…ì²˜**: ìƒˆë¡œìš´ AI ì„œë¹„ìŠ¤ ì‰½ê²Œ ì¶”ê°€
- ğŸ¯ **ì„œë¹„ìŠ¤ ì¶”ìƒí™”**: í†µì¼ëœ ì¸í„°í˜ì´ìŠ¤ë¡œ ë‹¤ì–‘í•œ AI ì„œë¹„ìŠ¤ ì‚¬ìš©
- ğŸ”„ **Fallback ë©”ì»¤ë‹ˆì¦˜**: ì„œë¹„ìŠ¤ ì¥ì•  ì‹œ ìë™ ì „í™˜
- ğŸ“Š **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: ê° AI ì„œë¹„ìŠ¤ì˜ ì„±ëŠ¥ ì¶”ì 
- ğŸ’° **ë¹„ìš© ìµœì í™”**: ì„œë¹„ìŠ¤ë³„ ë¹„ìš© ê´€ë¦¬ ë° ì œí•œ

## AI ì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜

### ì „ì²´ AI í”Œë¡œìš°

```mermaid
flowchart LR
  A[ì´ë¯¸ì§€ ì—…ë¡œë“œ] --> B[AI ì„œë¹„ìŠ¤ ì„ íƒ]
  B --> C[ì´ë¯¸ì§€ ë¶„ì„]
  C --> D[ê°ì • ì„ íƒ]
  D --> E[ìº¡ì…˜ ìƒì„±]
  E --> F[SEO ë©”íƒ€ë°ì´í„° ìƒì„±]
  F --> G[ìµœì¢… ì½˜í…ì¸ ]
  
  B --> H[Fallback ì²´í¬]
  H --> I[ëŒ€ì²´ ì„œë¹„ìŠ¤]
  I --> C
```

### ëª¨ë“ˆí™”ëœ AI ì„œë¹„ìŠ¤ êµ¬ì¡°

```
AI Services
â”œâ”€â”€ Core Interface
â”‚   â”œâ”€â”€ AIServiceProvider
â”‚   â”œâ”€â”€ AIServiceConfig
â”‚   â””â”€â”€ AIServiceResponse
â”œâ”€â”€ Providers
â”‚   â”œâ”€â”€ OpenAIProvider
â”‚   â”œâ”€â”€ AnthropicProvider
â”‚   â”œâ”€â”€ GoogleAIProvider
â”‚   â”œâ”€â”€ AzureOpenAIProvider
â”‚   â””â”€â”€ CustomProvider
â”œâ”€â”€ Services
â”‚   â”œâ”€â”€ CaptionGenerationService
â”‚   â”œâ”€â”€ ImageAnalysisService
â”‚   â””â”€â”€ SEOGenerationService
â””â”€â”€ Utils
    â”œâ”€â”€ PromptBuilder
    â”œâ”€â”€ ResponseParser
    â””â”€â”€ ErrorHandler
```

## AI ì„œë¹„ìŠ¤ ì¸í„°í˜ì´ìŠ¤

### ê¸°ë³¸ ì¸í„°í˜ì´ìŠ¤ ì •ì˜

```typescript
// types/AIService.ts
export interface AIServiceConfig {
  provider: string;
  apiKey: string;
  model: string;
  temperature?: number;
  maxTokens?: number;
  timeout?: number;
}

export interface AIServiceRequest {
  prompt: string;
  images?: string[]; // base64 encoded
  options?: {
    temperature?: number;
    maxTokens?: number;
    responseFormat?: 'text' | 'json';
  };
}

export interface AIServiceResponse {
  content: string;
  usage?: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
  metadata?: {
    model: string;
    provider: string;
    latency: number;
  };
}

export interface AIServiceProvider {
  name: string;
  config: AIServiceConfig;
  
  // ê¸°ë³¸ ë©”ì„œë“œ
  generateText(request: AIServiceRequest): Promise<AIServiceResponse>;
  analyzeImage(imageBase64: string, prompt: string): Promise<AIServiceResponse>;
  
  // ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ
  validateConfig(): boolean;
  getCostEstimate(tokens: number): number;
  isAvailable(): Promise<boolean>;
}
```

### ì„œë¹„ìŠ¤ íŒ©í† ë¦¬ íŒ¨í„´

```typescript
// services/AIServiceFactory.ts
export class AIServiceFactory {
  private static providers = new Map<string, AIServiceProvider>();
  
  static registerProvider(name: string, provider: AIServiceProvider): void {
    this.providers.set(name, provider);
  }
  
  static getProvider(name: string): AIServiceProvider | null {
    return this.providers.get(name) || null;
  }
  
  static getAvailableProviders(): string[] {
    return Array.from(this.providers.keys());
  }
  
  static async getBestProvider(serviceType: 'caption' | 'image-analysis'): Promise<AIServiceProvider> {
    const providers = Array.from(this.providers.values());
    
    // ê°€ìš©ì„± ì²´í¬
    const availableProviders = await Promise.all(
      providers.map(async (provider) => ({
        provider,
        available: await provider.isAvailable()
      }))
    );
    
    const available = availableProviders
      .filter(p => p.available)
      .map(p => p.provider);
    
    if (available.length === 0) {
      throw new Error('No available AI providers');
    }
    
    // ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì„ íƒ (ì„¤ì •ì—ì„œ ê´€ë¦¬)
    return available[0];
  }
}
```

## ì§€ì› AI ì„œë¹„ìŠ¤

### 1. OpenAI GPT-4o

```typescript
// providers/OpenAIProvider.ts
import OpenAI from 'openai';

export class OpenAIProvider implements AIServiceProvider {
  name = 'openai';
  private client: OpenAI;
  
  constructor(config: AIServiceConfig) {
    this.config = config;
    this.client = new OpenAI({
      apiKey: config.apiKey,
      dangerouslyAllowBrowser: false
    });
  }
  
  async generateText(request: AIServiceRequest): Promise<AIServiceResponse> {
    const startTime = Date.now();
    
    try {
      const completion = await this.client.chat.completions.create({
        model: this.config.model,
        messages: [
          {
            role: 'system',
            content: 'ë‹¹ì‹ ì€ ìˆ™ë°•ì—…ì†Œ SNS ë§ˆì¼€íŒ… ì „ë¬¸ê°€ì…ë‹ˆë‹¤.'
          },
          {
            role: 'user',
            content: request.prompt
          }
        ],
        temperature: request.options?.temperature || this.config.temperature || 0.7,
        max_tokens: request.options?.maxTokens || this.config.maxTokens || 500
      });
      
      const response = completion.choices[0]?.message?.content || '';
      const usage = completion.usage;
      
      return {
        content: response,
        usage: {
          promptTokens: usage?.prompt_tokens || 0,
          completionTokens: usage?.completion_tokens || 0,
          totalTokens: usage?.total_tokens || 0
        },
        metadata: {
          model: this.config.model,
          provider: this.name,
          latency: Date.now() - startTime
        }
      };
    } catch (error) {
      throw new AIError('OpenAI API í˜¸ì¶œ ì‹¤íŒ¨', 'OPENAI_ERROR', true);
    }
  }
  
  async analyzeImage(imageBase64: string, prompt: string): Promise<AIServiceResponse> {
    const startTime = Date.now();
    
    try {
      const completion = await this.client.chat.completions.create({
        model: this.config.model,
        messages: [
          {
            role: 'system',
            content: 'ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ìˆ™ë°•ì—…ì†Œ ë§ˆì¼€íŒ…ì— í•„ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.'
          },
          {
            role: 'user',
            content: [
              { type: 'text', text: prompt },
              {
                type: 'image_url',
                image_url: {
                  url: `data:image/jpeg;base64,${imageBase64}`
                }
              }
            ]
          }
        ],
        temperature: 0.3,
        max_tokens: 300
      });
      
      const response = completion.choices[0]?.message?.content || '';
      
      return {
        content: response,
        metadata: {
          model: this.config.model,
          provider: this.name,
          latency: Date.now() - startTime
        }
      };
    } catch (error) {
      throw new AIError('OpenAI ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨', 'OPENAI_IMAGE_ERROR', true);
    }
  }
  
  validateConfig(): boolean {
    return !!(this.config.apiKey && this.config.model);
  }
  
  getCostEstimate(tokens: number): number {
    // GPT-4o ë¹„ìš© ê³„ì‚° (ì‹¤ì œ ë¹„ìš©ìœ¼ë¡œ ìˆ˜ì • í•„ìš”)
    const costPer1kTokens = 0.005;
    return (tokens / 1000) * costPer1kTokens;
  }
  
  async isAvailable(): Promise<boolean> {
    try {
      // ê°„ë‹¨í•œ í—¬ìŠ¤ì²´í¬
      await this.client.models.list();
      return true;
    } catch {
      return false;
    }
  }
}
```

### 2. Anthropic Claude

```typescript
// providers/AnthropicProvider.ts
import Anthropic from '@anthropic-ai/sdk';

export class AnthropicProvider implements AIServiceProvider {
  name = 'anthropic';
  private client: Anthropic;
  
  constructor(config: AIServiceConfig) {
    this.config = config;
    this.client = new Anthropic({
      apiKey: config.apiKey
    });
  }
  
  async generateText(request: AIServiceRequest): Promise<AIServiceResponse> {
    const startTime = Date.now();
    
    try {
      const message = await this.client.messages.create({
        model: this.config.model,
        max_tokens: request.options?.maxTokens || this.config.maxTokens || 500,
        temperature: request.options?.temperature || this.config.temperature || 0.7,
        messages: [
          {
            role: 'user',
            content: request.prompt
          }
        ]
      });
      
      const response = message.content[0]?.text || '';
      
      return {
        content: response,
        metadata: {
          model: this.config.model,
          provider: this.name,
          latency: Date.now() - startTime
        }
      };
    } catch (error) {
      throw new AIError('Anthropic API í˜¸ì¶œ ì‹¤íŒ¨', 'ANTHROPIC_ERROR', true);
    }
  }
  
  // ì´ë¯¸ì§€ ë¶„ì„ì€ Claude 3.5 Sonnet ì´ìƒì—ì„œ ì§€ì›
  async analyzeImage(imageBase64: string, prompt: string): Promise<AIServiceResponse> {
    const startTime = Date.now();
    
    try {
      const message = await this.client.messages.create({
        model: this.config.model,
        max_tokens: 300,
        temperature: 0.3,
        messages: [
          {
            role: 'user',
            content: [
              { type: 'text', text: prompt },
              {
                type: 'image',
                source: {
                  type: 'base64',
                  media_type: 'image/jpeg',
                  data: imageBase64
                }
              }
            ]
          }
        ]
      });
      
      const response = message.content[0]?.text || '';
      
      return {
        content: response,
        metadata: {
          model: this.config.model,
          provider: this.name,
          latency: Date.now() - startTime
        }
      };
    } catch (error) {
      throw new AIError('Anthropic ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨', 'ANTHROPIC_IMAGE_ERROR', true);
    }
  }
  
  validateConfig(): boolean {
    return !!(this.config.apiKey && this.config.model);
  }
  
  getCostEstimate(tokens: number): number {
    // Claude ë¹„ìš© ê³„ì‚° (ì‹¤ì œ ë¹„ìš©ìœ¼ë¡œ ìˆ˜ì • í•„ìš”)
    const costPer1kTokens = 0.003;
    return (tokens / 1000) * costPer1kTokens;
  }
  
  async isAvailable(): Promise<boolean> {
    try {
      // ê°„ë‹¨í•œ í—¬ìŠ¤ì²´í¬
      await this.client.messages.create({
        model: this.config.model,
        max_tokens: 1,
        messages: [{ role: 'user', content: 'test' }]
      });
      return true;
    } catch {
      return false;
    }
  }
}
```

### 3. Google AI (Gemini)

```typescript
// providers/GoogleAIProvider.ts
import { GoogleGenerativeAI } from '@google/generative-ai';

export class GoogleAIProvider implements AIServiceProvider {
  name = 'google';
  private client: GoogleGenerativeAI;
  private model: any;
  
  constructor(config: AIServiceConfig) {
    this.config = config;
    this.client = new GoogleGenerativeAI(config.apiKey);
    this.model = this.client.getGenerativeModel({ model: config.model });
  }
  
  async generateText(request: AIServiceRequest): Promise<AIServiceResponse> {
    const startTime = Date.now();
    
    try {
      const result = await this.model.generateContent(request.prompt);
      const response = result.response.text();
      
      return {
        content: response,
        metadata: {
          model: this.config.model,
          provider: this.name,
          latency: Date.now() - startTime
        }
      };
    } catch (error) {
      throw new AIError('Google AI API í˜¸ì¶œ ì‹¤íŒ¨', 'GOOGLE_AI_ERROR', true);
    }
  }
  
  async analyzeImage(imageBase64: string, prompt: string): Promise<AIServiceResponse> {
    const startTime = Date.now();
    
    try {
      const imagePart = {
        inlineData: {
          data: imageBase64,
          mimeType: 'image/jpeg'
        }
      };
      
      const result = await this.model.generateContent([prompt, imagePart]);
      const response = result.response.text();
      
      return {
        content: response,
        metadata: {
          model: this.config.model,
          provider: this.name,
          latency: Date.now() - startTime
        }
      };
    } catch (error) {
      throw new AIError('Google AI ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨', 'GOOGLE_AI_IMAGE_ERROR', true);
    }
  }
  
  validateConfig(): boolean {
    return !!(this.config.apiKey && this.config.model);
  }
  
  getCostEstimate(tokens: number): number {
    // Gemini ë¹„ìš© ê³„ì‚° (ì‹¤ì œ ë¹„ìš©ìœ¼ë¡œ ìˆ˜ì • í•„ìš”)
    const costPer1kTokens = 0.001;
    return (tokens / 1000) * costPer1kTokens;
  }
  
  async isAvailable(): Promise<boolean> {
    try {
      await this.model.generateContent('test');
      return true;
    } catch {
      return false;
    }
  }
}
```

## ìƒˆë¡œìš´ AI ì„œë¹„ìŠ¤ ì¶”ê°€ ê°€ì´ë“œ

### 1. ê¸°ë³¸ êµ¬ì¡° ìƒì„±

ìƒˆë¡œìš´ AI ì„œë¹„ìŠ¤ë¥¼ ì¶”ê°€í•˜ë ¤ë©´ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¥´ì„¸ìš”:

#### Step 1: Provider í´ë˜ìŠ¤ ìƒì„±

```typescript
// providers/YourAIProvider.ts
import { AIServiceProvider, AIServiceConfig, AIServiceRequest, AIServiceResponse } from '../types/AIService';

export class YourAIProvider implements AIServiceProvider {
  name = 'your-ai';
  config: AIServiceConfig;
  
  constructor(config: AIServiceConfig) {
    this.config = config;
  }
  
  async generateText(request: AIServiceRequest): Promise<AIServiceResponse> {
    // êµ¬í˜„ ë‚´ìš©
  }
  
  async analyzeImage(imageBase64: string, prompt: string): Promise<AIServiceResponse> {
    // êµ¬í˜„ ë‚´ìš©
  }
  
  validateConfig(): boolean {
    // ì„¤ì • ê²€ì¦ ë¡œì§
  }
  
  getCostEstimate(tokens: number): number {
    // ë¹„ìš© ê³„ì‚° ë¡œì§
  }
  
  async isAvailable(): Promise<boolean> {
    // ê°€ìš©ì„± ì²´í¬ ë¡œì§
  }
}
```

#### Step 2: í™˜ê²½ ë³€ìˆ˜ ì¶”ê°€

```env
# .env.local
YOUR_AI_API_KEY=your_api_key_here
YOUR_AI_MODEL=your_model_name
```

#### Step 3: ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸

```typescript
// config/aiConfig.ts
export const aiConfig = {
  providers: {
    openai: {
      apiKey: process.env.OPENAI_API_KEY,
      model: 'gpt-4o',
      temperature: 0.7,
      maxTokens: 500
    },
    anthropic: {
      apiKey: process.env.ANTHROPIC_API_KEY,
      model: 'claude-3-5-sonnet-20241022',
      temperature: 0.7,
      maxTokens: 500
    },
    google: {
      apiKey: process.env.GOOGLE_AI_API_KEY,
      model: 'gemini-1.5-pro',
      temperature: 0.7,
      maxTokens: 500
    },
    yourAI: {
      apiKey: process.env.YOUR_AI_API_KEY,
      model: process.env.YOUR_AI_MODEL,
      temperature: 0.7,
      maxTokens: 500
    }
  },
  fallback: {
    primary: 'openai',
    secondary: 'anthropic',
    tertiary: 'google'
  }
};
```

#### Step 4: Provider ë“±ë¡

```typescript
// services/AIServiceRegistry.ts
import { AIServiceFactory } from './AIServiceFactory';
import { YourAIProvider } from '../providers/YourAIProvider';
import { aiConfig } from '../config/aiConfig';

export function registerAIProviders(): void {
  // OpenAI ë“±ë¡
  const openaiProvider = new OpenAIProvider(aiConfig.providers.openai);
  AIServiceFactory.registerProvider('openai', openaiProvider);
  
  // Anthropic ë“±ë¡
  const anthropicProvider = new AnthropicProvider(aiConfig.providers.anthropic);
  AIServiceFactory.registerProvider('anthropic', anthropicProvider);
  
  // Google AI ë“±ë¡
  const googleProvider = new GoogleAIProvider(aiConfig.providers.google);
  AIServiceFactory.registerProvider('google', googleProvider);
  
  // ìƒˆë¡œìš´ AI ì„œë¹„ìŠ¤ ë“±ë¡
  const yourAIProvider = new YourAIProvider(aiConfig.providers.yourAI);
  AIServiceFactory.registerProvider('your-ai', yourAIProvider);
}
```

#### Step 5: Supabase Function ì—…ë°ì´íŠ¸

```typescript
// supabase/functions/generate-caption/index.ts
import { AIServiceFactory } from '../../../src/services/AIServiceFactory';
import { registerAIProviders } from '../../../src/services/AIServiceRegistry';

// Provider ë“±ë¡
registerAIProviders();

Deno.serve(async (req: Request) => {
  // ... ê¸°ì¡´ ì½”ë“œ ...
  
  try {
    // ìµœì ì˜ AI ì„œë¹„ìŠ¤ ì„ íƒ
    const provider = await AIServiceFactory.getBestProvider('caption');
    
    const result = await provider.generateText({
      prompt: buildCaptionPrompt({ emotion, templateId, storeName, placeDesc }),
      options: {
        temperature: 0.7,
        maxTokens: 500,
        responseFormat: 'text'
      }
    });
    
    // ì‘ë‹µ íŒŒì‹±
    const parsedResult = parseCaptionResponse(result.content);
    
    return new Response(JSON.stringify(parsedResult), {
      headers: { ...corsHeaders, "Content-Type": "application/json" },
      status: 200,
    });
  } catch (e) {
    // Fallback ì²˜ë¦¬
    console.error("[generate-caption] ERROR:", e);
    
    // ëŒ€ì²´ ì„œë¹„ìŠ¤ë¡œ ì¬ì‹œë„
    try {
      const fallbackProvider = AIServiceFactory.getProvider('anthropic');
      if (fallbackProvider) {
        const result = await fallbackProvider.generateText({
          prompt: buildCaptionPrompt({ emotion, templateId, storeName, placeDesc })
        });
        
        const parsedResult = parseCaptionResponse(result.content);
        
        return new Response(JSON.stringify(parsedResult), {
          headers: { ...corsHeaders, "Content-Type": "application/json" },
          status: 200,
        });
      }
    } catch (fallbackError) {
      console.error("[generate-caption] FALLBACK ERROR:", fallbackError);
    }
    
    return new Response(JSON.stringify({ error: "INTERNAL_ERROR", message: String(e) }), {
      headers: { ...corsHeaders, "Content-Type": "application/json" },
      status: 500,
    });
  }
});
```

### 2. í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

```typescript
// tests/YourAIProvider.test.ts
import { YourAIProvider } from '../providers/YourAIProvider';

describe('YourAIProvider', () => {
  let provider: YourAIProvider;
  
  beforeEach(() => {
    provider = new YourAIProvider({
      provider: 'your-ai',
      apiKey: 'test-key',
      model: 'test-model'
    });
  });
  
  test('should validate config correctly', () => {
    expect(provider.validateConfig()).toBe(true);
  });
  
  test('should generate text', async () => {
    const result = await provider.generateText({
      prompt: 'í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸'
    });
    
    expect(result.content).toBeDefined();
    expect(result.metadata).toBeDefined();
  });
  
  test('should analyze image', async () => {
    const testImage = 'base64-encoded-image-data';
    const result = await provider.analyzeImage(testImage, 'ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”');
    
    expect(result.content).toBeDefined();
  });
});
```

## í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§

### í†µí•© í”„ë¡¬í”„íŠ¸ ë¹Œë”

```typescript
// utils/PromptBuilder.ts
export class PromptBuilder {
  private static emotionContexts = {
    'ì„¤ë ˜': 'ê¸°ëŒ€ê°ê³¼ ì„¤ë ˜ì„ ëŠë‚„ ìˆ˜ ìˆëŠ” ë¶„ìœ„ê¸°',
    'í‰ì˜¨': 'ì°¨ë¶„í•˜ê³  í‰í™”ë¡œìš´ ë¶„ìœ„ê¸°',
    'ì¦ê±°ì›€': 'í™œê¸°ì°¨ê³  ì¦ê±°ìš´ ë¶„ìœ„ê¸°',
    'ë¡œë§¨í‹±': 'ë¡œë§¨í‹±í•˜ê³  ì•„ë¦„ë‹¤ìš´ ë¶„ìœ„ê¸°',
    'íë§': 'í¸ì•ˆí•˜ê³  íë§ë˜ëŠ” ë¶„ìœ„ê¸°'
  };
  
  private static templateStyles = {
    'default_universal': 'ì¼ë°˜ì ì¸ SNS ìŠ¤íƒ€ì¼',
    'ocean_sunset': 'ì˜¤ì…˜ ì„ ì…‹ ë¶„ìœ„ê¸°',
    'luxury_pool': 'ëŸ­ì…”ë¦¬ í’€ ë¶„ìœ„ê¸°',
    'cafe_cozy': 'ì¹´í˜ ì½”ì§€ ë¶„ìœ„ê¸°'
  };
  
  static buildCaptionPrompt(params: {
    emotion: string;
    templateId: string;
    storeName: string;
    placeDesc?: string;
    provider?: string;
  }): string {
    const { emotion, templateId, storeName, placeDesc, provider } = params;
    
    const emotionContext = this.emotionContexts[emotion] || this.emotionContexts['í‰ì˜¨'];
    const templateStyle = this.templateStyles[templateId] || this.templateStyles['default_universal'];
    
    // Providerë³„ ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸
    const providerSpecificPrompt = this.getProviderSpecificPrompt(provider);
    
    return `
ìˆ™ë°•ì—…ì†Œ "${storeName}"ì˜ SNS ìº¡ì…˜ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

ê°ì •: ${emotion}
${emotionContext}

ìŠ¤íƒ€ì¼: ${templateStyle}

${placeDesc ? `ì¥ì†Œ ì„¤ëª…: ${placeDesc}` : ''}

${providerSpecificPrompt}

ìš”êµ¬ì‚¬í•­:
1. ê°ì •ì— ë§ëŠ” í†¤ì•¤ë§¤ë„ˆë¡œ ì‘ì„±
2. 2-3ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±
3. ì´ëª¨ì§€ 1-2ê°œ í¬í•¨
4. í•´ì‹œíƒœê·¸ 3-5ê°œ ìƒì„±

ì¶œë ¥ í˜•ì‹:
í›…: [ë§¤ë ¥ì ì¸ ì²« ë¬¸ì¥]
ìº¡ì…˜: [ë³¸ë¬¸ ë‚´ìš©]
í•´ì‹œíƒœê·¸: [í•´ì‹œíƒœê·¸ ëª©ë¡]
`;
  }
  
  private static getProviderSpecificPrompt(provider?: string): string {
    switch (provider) {
      case 'openai':
        return 'OpenAI GPT ëª¨ë¸ì˜ íŠ¹ì„±ì„ í™œìš©í•˜ì—¬ ì°½ì˜ì ì´ê³  ë§¤ë ¥ì ì¸ ìº¡ì…˜ì„ ìƒì„±í•´ì£¼ì„¸ìš”.';
      case 'anthropic':
        return 'Claudeì˜ ì•ˆì „í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” íŠ¹ì„±ì„ í™œìš©í•˜ì—¬ ì ì ˆí•˜ê³  ë§¤ë ¥ì ì¸ ìº¡ì…˜ì„ ìƒì„±í•´ì£¼ì„¸ìš”.';
      case 'google':
        return 'Geminiì˜ ë‹¤ì¬ë‹¤ëŠ¥í•œ íŠ¹ì„±ì„ í™œìš©í•˜ì—¬ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ë§¤ë ¥ì ì¸ ìº¡ì…˜ì„ ìƒì„±í•´ì£¼ì„¸ìš”.';
      default:
        return '';
    }
  }
  
  static buildImageAnalysisPrompt(): string {
    return `
ë‹¹ì‹ ì€ í•œêµ­ì˜ íœì…˜/ìˆ™ë°•ì—…ì†Œ ì „ë¬¸ ë§ˆì¼€íŒ… ë¶„ì„ê°€ì…ë‹ˆë‹¤.
ì—…ë¡œë“œëœ ìˆ™ì†Œ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì •ë³´ë¥¼ JSON í˜•íƒœë¡œ ì œê³µí•´ì£¼ì„¸ìš”:

1. main_features: ì´ë¯¸ì§€ì—ì„œ ë³´ì´ëŠ” ì£¼ìš” íŠ¹ì§•ë“¤ (ìµœëŒ€ 5ê°œ, í•œêµ­ì–´)
   ì˜ˆ: ["ë°”ë‹¤", "ìˆ˜ì˜ì¥", "ë…¸ì„", "ì‚°", "ì •ì›", "í…Œë¼ìŠ¤", "ë°”ë² íì‹œì„¤", "í‚¤ì¦ˆí’€", "ìì¿ ì§€"]

2. view_type: ìˆ™ì†Œì˜ ë·° íƒ€ì… (í•œêµ­ì–´)
   ì˜ˆ: "ì˜¤ì…˜ë·°", "ë§ˆìš´í‹´ë·°", "ì‹œí‹°ë·°", "ê°€ë“ ë·°", "ë¦¬ë²„ë·°", "ë…¼ë·°", "í¬ë ˆìŠ¤íŠ¸ë·°", "ë ˆì´í¬ë·°"

3. emotions: ì´ ìˆ™ì†Œê°€ ìê·¹í•˜ëŠ” ê°ì„± í‚¤ì›Œë“œ (ìµœëŒ€ 3ê°œ, í•œêµ­ì–´)
   ì˜ˆ: ["ê°ì„± íë§", "ëŸ­ì…”ë¦¬í•¨", "ì—¬ìœ ë¡œì›€", "ë¡œë§¨í‹±", "ê°€ì¡±ì¹œí™”", "ê³ ìš”í•¨", "ëª¨ë˜í•¨", "ì•„ëŠ‘í•¨"]

4. hashtags: ì¸ìŠ¤íƒ€ê·¸ë¨ìš© í•´ì‹œíƒœê·¸ (5-8ê°œ, í•œêµ­ì–´)
   ì§€ì—­ëª…, ìˆ™ì†Œíƒ€ì…, íŠ¹ì§•ì„ í¬í•¨í•˜ì—¬ ì‹¤ì œ ë§ˆì¼€íŒ…ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í•´ì‹œíƒœê·¸
   ì˜ˆ: ["#ì œì£¼ë„íœì…˜", "#ì˜¤ì…˜ë·°ìˆ™ì†Œ", "#í’€ë¹Œë¼ì¶”ì²œ", "#ê°ì„±ìˆ™ì†Œ", "#ì»¤í”Œì—¬í–‰"]

ë°˜ë“œì‹œ ë‹¤ìŒ JSON êµ¬ì¡°ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{
  "main_features": ["íŠ¹ì§•1", "íŠ¹ì§•2", "íŠ¹ì§•3"],
  "view_type": "ë·°íƒ€ì…",
  "emotions": ["ê°ì„±1", "ê°ì„±2"],
  "hashtags": ["#í•´ì‹œíƒœê·¸1", "#í•´ì‹œíƒœê·¸2", "#í•´ì‹œíƒœê·¸3", "#í•´ì‹œíƒœê·¸4", "#í•´ì‹œíƒœê·¸5"]
}
`;
  }
}
```

## ì´ë¯¸ì§€ ë¶„ì„

### í†µí•© ì´ë¯¸ì§€ ë¶„ì„ ì„œë¹„ìŠ¤

```typescript
// services/ImageAnalysisService.ts
export class ImageAnalysisService {
  private static instance: ImageAnalysisService;
  private cache = new Map<string, any>();
  
  static getInstance(): ImageAnalysisService {
    if (!this.instance) {
      this.instance = new ImageAnalysisService();
    }
    return this.instance;
  }
  
  async analyzeImage(imageBase64: string): Promise<ImageMeta> {
    // ìºì‹œ í™•ì¸
    const cacheKey = this.generateCacheKey(imageBase64);
    if (this.cache.has(cacheKey)) {
      return this.cache.get(cacheKey);
    }
    
    try {
      // ìµœì ì˜ AI ì„œë¹„ìŠ¤ ì„ íƒ
      const provider = await AIServiceFactory.getBestProvider('image-analysis');
      
      const prompt = PromptBuilder.buildImageAnalysisPrompt();
      
      const result = await provider.analyzeImage(imageBase64, prompt);
      
      // ì‘ë‹µ íŒŒì‹±
      const imageMeta = this.parseImageMetaResponse(result.content);
      
      // ìºì‹œì— ì €ì¥
      this.cache.set(cacheKey, imageMeta);
      
      return imageMeta;
    } catch (error) {
      console.error('Image analysis failed:', error);
      
      // Fallback ì²˜ë¦¬
      return this.getFallbackImageMeta();
    }
  }
  
  private generateCacheKey(imageBase64: string): string {
    // ê°„ë‹¨í•œ í•´ì‹œ ìƒì„± (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ í•´ì‹œ ì‚¬ìš©)
    return btoa(imageBase64.substring(0, 100));
  }
  
  private parseImageMetaResponse(response: string): ImageMeta {
    try {
      // JSON íŒŒì‹± ì‹œë„
      const jsonMatch = response.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        return JSON.parse(jsonMatch[0]);
      }
      
      // í…ìŠ¤íŠ¸ íŒŒì‹±
      const lines = response.split('\n');
      const result = {
        main_features: [],
        view_type: '',
        emotions: [],
        hashtags: []
      };
      
      for (const line of lines) {
        if (line.includes('ì£¼ìš” íŠ¹ì§•:')) {
          result.main_features = line.split(':')[1]?.split(',').map(f => f.trim()) || [];
        } else if (line.includes('ë·° íƒ€ì…:')) {
          result.view_type = line.split(':')[1]?.trim() || '';
        } else if (line.includes('ê°ì •:')) {
          result.emotions = line.split(':')[1]?.split(',').map(e => e.trim()) || [];
        } else if (line.includes('í•´ì‹œíƒœê·¸:')) {
          result.hashtags = line.split(':')[1]?.split(',').map(h => h.trim()) || [];
        }
      }
      
      return result;
    } catch (error) {
      console.error('Image meta parsing error:', error);
      return this.getFallbackImageMeta();
    }
  }
  
  private getFallbackImageMeta(): ImageMeta {
    return {
      main_features: ['ìˆ™ë°•ì—…ì†Œ'],
      view_type: 'ì¼ë°˜',
      emotions: ['í¸ì•ˆí•¨'],
      hashtags: ['#ìˆ™ë°•ì—…ì†Œ', '#ì—¬í–‰']
    };
  }
}
```

## ì—ëŸ¬ ì²˜ë¦¬

### í†µí•© ì—ëŸ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ

```typescript
// utils/AIErrorHandler.ts
export class AIError extends Error {
  constructor(
    message: string,
    public code: string,
    public retryable: boolean = false,
    public provider?: string
  ) {
    super(message);
    this.name = 'AIError';
  }
}

export class AIErrorHandler {
  private static errorMap = {
    'INVALID_API_KEY': {
      message: 'API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.',
      retryable: false,
      action: 'API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.'
    },
    'RATE_LIMIT': {
      message: 'ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.',
      retryable: true,
      action: 'ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.'
    },
    'QUOTA_EXCEEDED': {
      message: 'ì‚¬ìš©ëŸ‰ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.',
      retryable: false,
      action: 'ë‹¤ìŒ ë‹¬ê¹Œì§€ ê¸°ë‹¤ë¦¬ê±°ë‚˜ ë‹¤ë¥¸ ì„œë¹„ìŠ¤ë¡œ ì „í™˜í•´ì£¼ì„¸ìš”.'
    },
    'MODEL_NOT_FOUND': {
      message: 'ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.',
      retryable: false,
      action: 'ëª¨ë¸ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.'
    },
    'NETWORK_ERROR': {
      message: 'ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.',
      retryable: true,
      action: 'ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.'
    },
    'TIMEOUT': {
      message: 'ìš”ì²­ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.',
      retryable: true,
      action: 'ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.'
    }
  };
  
  static handleError(error: any, provider?: string): AIError {
    if (error instanceof AIError) {
      return error;
    }
    
    // Providerë³„ ì—ëŸ¬ ì²˜ë¦¬
    const providerError = this.handleProviderSpecificError(error, provider);
    if (providerError) {
      return providerError;
    }
    
    // ì¼ë°˜ì ì¸ ì—ëŸ¬ ì²˜ë¦¬
    if (error?.response?.status) {
      return this.handleHTTPError(error.response.status, provider);
    }
    
    // ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬
    if (error.name === 'TypeError' && error.message.includes('fetch')) {
      return new AIError(
        this.errorMap.NETWORK_ERROR.message,
        'NETWORK_ERROR',
        true,
        provider
      );
    }
    
    return new AIError(
      'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
      'UNKNOWN_ERROR',
      false,
      provider
    );
  }
  
  private static handleProviderSpecificError(error: any, provider?: string): AIError | null {
    switch (provider) {
      case 'openai':
        return this.handleOpenAIError(error);
      case 'anthropic':
        return this.handleAnthropicError(error);
      case 'google':
        return this.handleGoogleAIError(error);
      default:
        return null;
    }
  }
  
  private static handleOpenAIError(error: any): AIError | null {
    if (error?.response?.data?.error?.type) {
      const errorType = error.response.data.error.type;
      
      switch (errorType) {
        case 'invalid_request_error':
          return new AIError('ì˜ëª»ëœ ìš”ì²­ì…ë‹ˆë‹¤.', 'INVALID_REQUEST', false, 'openai');
        case 'authentication_error':
          return new AIError('ì¸ì¦ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.', 'AUTH_ERROR', false, 'openai');
        case 'rate_limit_error':
          return new AIError('ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.', 'RATE_LIMIT', true, 'openai');
        default:
          return null;
      }
    }
    return null;
  }
  
  private static handleAnthropicError(error: any): AIError | null {
    // Anthropic íŠ¹í™” ì—ëŸ¬ ì²˜ë¦¬
    return null;
  }
  
  private static handleGoogleAIError(error: any): AIError | null {
    // Google AI íŠ¹í™” ì—ëŸ¬ ì²˜ë¦¬
    return null;
  }
  
  private static handleHTTPError(status: number, provider?: string): AIError {
    switch (status) {
      case 401:
        return new AIError('API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.', 'INVALID_API_KEY', false, provider);
      case 429:
        return new AIError('ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.', 'RATE_LIMIT', true, provider);
      case 500:
        return new AIError('AI ì„œë¹„ìŠ¤ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.', 'AI_SERVICE_ERROR', true, provider);
      default:
        return new AIError('AI ì„œë¹„ìŠ¤ í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.', 'UNKNOWN_ERROR', false, provider);
    }
  }
}
```

## ì„±ëŠ¥ ìµœì í™”

### ìºì‹± ë° ë°°ì¹˜ ì²˜ë¦¬

```typescript
// utils/AIPerformanceOptimizer.ts
export class AIPerformanceOptimizer {
  private static cache = new Map<string, { data: any; timestamp: number; ttl: number }>();
  private static batchQueue: Array<() => Promise<any>> = [];
  private static processing = false;
  
  static async getCachedResult<T>(key: string, ttl: number = 5 * 60 * 1000): Promise<T | null> {
    const item = this.cache.get(key);
    if (!item) return null;
    
    if (Date.now() - item.timestamp > item.ttl) {
      this.cache.delete(key);
      return null;
    }
    
    return item.data as T;
  }
  
  static setCachedResult(key: string, data: any, ttl: number = 5 * 60 * 1000): void {
    this.cache.set(key, {
      data,
      timestamp: Date.now(),
      ttl
    });
  }
  
  static async batchProcess<T>(tasks: Array<() => Promise<T>>, batchSize: number = 5): Promise<T[]> {
    const results: T[] = [];
    
    for (let i = 0; i < tasks.length; i += batchSize) {
      const batch = tasks.slice(i, i + batchSize);
      const batchResults = await Promise.all(batch.map(task => task()));
      results.push(...batchResults);
      
      // ë°°ì¹˜ ê°„ ë”œë ˆì´
      if (i + batchSize < tasks.length) {
        await new Promise(resolve => setTimeout(resolve, 1000));
      }
    }
    
    return results;
  }
  
  static clearCache(): void {
    this.cache.clear();
  }
}
```

## ë¹„ìš© ê´€ë¦¬

### í†µí•© ë¹„ìš© ê´€ë¦¬ ì‹œìŠ¤í…œ

```typescript
// utils/AICostManager.ts
export class AICostManager {
  private static usage = new Map<string, {
    totalTokens: number;
    totalCost: number;
    requests: number;
    dailyCost: number;
    lastReset: string;
  }>();
  
  private static readonly dailyLimits = {
    openai: 10, // $10
    anthropic: 8, // $8
    google: 5, // $5
    default: 5
  };
  
  static recordUsage(provider: string, tokens: number, cost: number): void {
    const today = new Date().toDateString();
    const current = this.usage.get(provider) || {
      totalTokens: 0,
      totalCost: 0,
      requests: 0,
      dailyCost: 0,
      lastReset: today
    };
    
    // ì¼ì¼ ë¦¬ì…‹ ì²´í¬
    if (current.lastReset !== today) {
      current.dailyCost = 0;
      current.lastReset = today;
    }
    
    current.totalTokens += tokens;
    current.totalCost += cost;
    current.dailyCost += cost;
    current.requests += 1;
    
    this.usage.set(provider, current);
    
    // ë¡œê¹…
    console.log(`[${provider}] Usage: ${tokens} tokens, Cost: $${cost.toFixed(4)}, Daily: $${current.dailyCost.toFixed(4)}`);
  }
  
  static canMakeRequest(provider: string, estimatedCost: number): boolean {
    const limit = this.dailyLimits[provider] || this.dailyLimits.default;
    const current = this.usage.get(provider);
    
    if (!current) return true;
    
    const today = new Date().toDateString();
    if (current.lastReset !== today) {
      return true;
    }
    
    return current.dailyCost + estimatedCost <= limit;
  }
  
  static getUsage(provider?: string): any {
    if (provider) {
      return this.usage.get(provider);
    }
    
    return Object.fromEntries(this.usage);
  }
  
  static getDailyUsage(provider: string): { cost: number; limit: number; remaining: number } {
    const limit = this.dailyLimits[provider] || this.dailyLimits.default;
    const current = this.usage.get(provider);
    
    if (!current) {
      return { cost: 0, limit, remaining: limit };
    }
    
    const today = new Date().toDateString();
    const dailyCost = current.lastReset === today ? current.dailyCost : 0;
    
    return {
      cost: dailyCost,
      limit,
      remaining: limit - dailyCost
    };
  }
  
  static resetUsage(): void {
    this.usage.clear();
  }
}
```

## ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

### AI ì„œë¹„ìŠ¤ ëª¨ë‹ˆí„°ë§

```typescript
// utils/AIMonitor.ts
export class AIMonitor {
  private static metrics = {
    requests: 0,
    errors: 0,
    totalLatency: 0,
    providerUsage: new Map<string, number>()
  };
  
  static recordRequest(provider: string, latency: number, success: boolean): void {
    this.metrics.requests += 1;
    this.metrics.totalLatency += latency;
    
    if (!success) {
      this.metrics.errors += 1;
    }
    
    const currentUsage = this.metrics.providerUsage.get(provider) || 0;
    this.metrics.providerUsage.set(provider, currentUsage + 1);
    
    // ë¡œê¹…
    console.log(`[AIMonitor] ${provider}: ${success ? 'SUCCESS' : 'ERROR'}, ${latency}ms`);
  }
  
  static getMetrics(): any {
    const avgLatency = this.metrics.requests > 0 
      ? this.metrics.totalLatency / this.metrics.requests 
      : 0;
    
    return {
      requests: this.metrics.requests,
      errors: this.metrics.errors,
      errorRate: this.metrics.requests > 0 
        ? (this.metrics.errors / this.metrics.requests) * 100 
        : 0,
      averageLatency: avgLatency,
      providerUsage: Object.fromEntries(this.metrics.providerUsage)
    };
  }
  
  static resetMetrics(): void {
    this.metrics = {
      requests: 0,
      errors: 0,
      totalLatency: 0,
      providerUsage: new Map()
    };
  }
}
```

## ğŸ›ï¸ ADR (Architecture Decision Records)

### ADR-001: ëª¨ë“ˆí™”ëœ AI ì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜
**ë‚ ì§œ**: 2025-01-14  
**ìƒíƒœ**: ìŠ¹ì¸ë¨  
**ì»¨í…ìŠ¤íŠ¸**: ë‹¤ì–‘í•œ AI ì„œë¹„ìŠ¤ í†µí•©ì„ ìœ„í•œ ì•„í‚¤í…ì²˜ ì„ íƒ  
**ê²°ì •**: Provider íŒ¨í„´ê³¼ Factory íŒ¨í„´ì„ ì‚¬ìš©í•œ ëª¨ë“ˆí™”ëœ ì•„í‚¤í…ì²˜ ì±„íƒ  
**ê²°ê³¼**: ìƒˆë¡œìš´ AI ì„œë¹„ìŠ¤ ì‰½ê²Œ ì¶”ê°€ ê°€ëŠ¥, ì„œë¹„ìŠ¤ ê°„ ì „í™˜ ìš©ì´

### ADR-002: Fallback ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„
**ë‚ ì§œ**: 2025-01-14  
**ìƒíƒœ**: ìŠ¹ì¸ë¨  
**ì»¨í…ìŠ¤íŠ¸**: AI ì„œë¹„ìŠ¤ ì¥ì•  ì‹œ ëŒ€ì‘ ë°©ì•ˆ  
**ê²°ì •**: ë‹¤ì¤‘ ì„œë¹„ìŠ¤ Fallback ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„  
**ê²°ê³¼**: ì„œë¹„ìŠ¤ ì•ˆì •ì„± í–¥ìƒ, ì‚¬ìš©ì ê²½í—˜ ê°œì„ 

### ADR-003: í†µí•© ë¹„ìš© ê´€ë¦¬ ì‹œìŠ¤í…œ
**ë‚ ì§œ**: 2025-01-14  
**ìƒíƒœ**: ìŠ¹ì¸ë¨  
**ì»¨í…ìŠ¤íŠ¸**: ë‹¤ì–‘í•œ AI ì„œë¹„ìŠ¤ì˜ ë¹„ìš© ê´€ë¦¬  
**ê²°ì •**: ì¤‘ì•™í™”ëœ ë¹„ìš© ê´€ë¦¬ ì‹œìŠ¤í…œ êµ¬í˜„  
**ê²°ê³¼**: ë¹„ìš© ì¶”ì  ë° ì œí•œ ê¸°ëŠ¥ ì œê³µ

## ğŸ“‹ Changelog

| ë‚ ì§œ | ë²„ì „ | ìš”ì•½ |
|------|------|------|
| 2025-01-14 | v2.0.0 | ëª¨ë“ˆí™”ëœ AI ì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ë¡œ ì™„ì „ ì¬ì‘ì„± |
| 2025-01-14 | v2.1.0 | Anthropic, Google AI ì§€ì› ì¶”ê°€ |
| 2025-01-14 | v2.2.0 | Fallback ë©”ì»¤ë‹ˆì¦˜ ë° ë¹„ìš© ê´€ë¦¬ ì‹œìŠ¤í…œ ì¶”ê°€ |
| 2025-01-14 | v2.3.0 | ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹… ì‹œìŠ¤í…œ ì¶”ê°€ |

## ğŸ”® í–¥í›„ ê³„íš

### ë‹¨ê¸° ê³„íš (1-3ê°œì›”)
- [ ] Azure OpenAI ì§€ì› ì¶”ê°€
- [ ] ë¡œì»¬ AI ëª¨ë¸ ì§€ì› (Ollama ë“±)
- [ ] ì‹¤ì‹œê°„ ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ êµ¬í˜„
- [ ] A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ êµ¬ì¶•

### ì¤‘ê¸° ê³„íš (3-6ê°œì›”)
- [ ] ë©€í‹°ëª¨ë‹¬ AI ì„œë¹„ìŠ¤ í†µí•©
- [ ] ìë™ í”„ë¡¬í”„íŠ¸ ìµœì í™”
- [ ] ì‚¬ìš©ìë³„ AI ì„œë¹„ìŠ¤ ì„ í˜¸ë„ í•™ìŠµ
- [ ] ë¹„ìš© ì˜ˆì¸¡ ë° ì•Œë¦¼ ì‹œìŠ¤í…œ

### ì¥ê¸° ê³„íš (6ê°œì›” ì´ìƒ)
- [ ] ìì²´ AI ëª¨ë¸ ê°œë°œ ê²€í† 
- [ ] ì—£ì§€ ì»´í“¨íŒ… ê¸°ë°˜ AI ì²˜ë¦¬
- [ ] ì‹¤ì‹œê°„ AI ì„œë¹„ìŠ¤ ì„±ëŠ¥ ìµœì í™”
- [ ] ê¸€ë¡œë²Œ AI ì„œë¹„ìŠ¤ í™•ì¥
